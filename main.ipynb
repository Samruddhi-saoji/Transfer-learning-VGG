{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Cats and dogs classification using VGG16 <br>\n",
    "Transfer learning: feature extraction technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Sequential #model\n",
    "from keras.layers import Dense, Flatten #layers\n",
    "from keras.applications.vgg16 import VGG16 #pre-trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the dataset : cats and dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7998 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#image size = (64, 64)\n",
    "dataset_path = 'C:\\\\Users\\\\Samruddhi\\\\Desktop\\\\Neural Networks\\\\datasets\\\\cats and dogs dataset\\\\dataset'\n",
    "\n",
    "\n",
    "############# data augmentation #########################\n",
    "\n",
    "#training set\n",
    "train_augmentor = ImageDataGenerator(rescale = 1./255, \n",
    " shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "\n",
    "training_set = train_augmentor.flow_from_directory(dataset_path + '\\\\training_set', target_size = (64, 64), batch_size = 32, class_mode = 'binary')\n",
    "\n",
    "\n",
    "#validation set\n",
    "#no data augmentation except for feature scaling\n",
    "validation_augmentor = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "validation_set = validation_augmentor.flow_from_directory(dataset_path +'\\\\test_set', target_size = (64, 64), batch_size = 32, class_mode = 'binary')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained model : VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolution base of vgg16\n",
    "conv_base = VGG16(\n",
    "    weights = \"imagenet\", #use the same weights as obtained from tarining on imagenet dataset\n",
    "    include_top = False, #dont include the flattening, ANN part\n",
    "    input_shape = (64, 64, 3)\n",
    "    )\n",
    "\n",
    "#print( conv_base.summary) #returns details abt the layers in conv_base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x0000015B434B9280>> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "############# add the layers #############################\n",
    "model.add(conv_base) #first add the convo base\n",
    "model.add(Flatten()) #flattening layer\n",
    "#now add the ANN part\n",
    "model.add(Dense(128, activation = \"relu\")) \n",
    "model.add(Dense(1, activation = \"sigmoid\")) #output layer\n",
    "\n",
    "print(model.summary , \"\\n\") #details abt the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samruddhi\\AppData\\Local\\Temp\\ipykernel_10332\\4007629962.py:10: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(training_set, validation_data=validation_set, epochs = 15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 112s 446ms/step - loss: 0.4449 - accuracy: 0.7899 - val_loss: 0.4254 - val_accuracy: 0.7995\n",
      "Epoch 2/15\n",
      "250/250 [==============================] - 128s 512ms/step - loss: 0.4315 - accuracy: 0.7929 - val_loss: 0.4221 - val_accuracy: 0.8020\n",
      "Epoch 3/15\n",
      "250/250 [==============================] - 140s 560ms/step - loss: 0.4129 - accuracy: 0.8085 - val_loss: 0.4143 - val_accuracy: 0.8015\n",
      "Epoch 4/15\n",
      "250/250 [==============================] - 178s 714ms/step - loss: 0.4131 - accuracy: 0.8056 - val_loss: 0.4294 - val_accuracy: 0.7870\n",
      "Epoch 5/15\n",
      "250/250 [==============================] - 112s 447ms/step - loss: 0.4041 - accuracy: 0.8108 - val_loss: 0.4093 - val_accuracy: 0.8040\n",
      "Epoch 6/15\n",
      "250/250 [==============================] - 139s 555ms/step - loss: 0.3880 - accuracy: 0.8191 - val_loss: 0.4078 - val_accuracy: 0.8080\n",
      "Epoch 7/15\n",
      "250/250 [==============================] - 194s 775ms/step - loss: 0.3946 - accuracy: 0.8175 - val_loss: 0.4009 - val_accuracy: 0.8135\n",
      "Epoch 8/15\n",
      "250/250 [==============================] - 111s 445ms/step - loss: 0.3727 - accuracy: 0.8316 - val_loss: 0.4490 - val_accuracy: 0.7780\n",
      "Epoch 9/15\n",
      "250/250 [==============================] - 112s 447ms/step - loss: 0.3719 - accuracy: 0.8278 - val_loss: 0.4017 - val_accuracy: 0.8145\n",
      "Epoch 10/15\n",
      "250/250 [==============================] - 114s 455ms/step - loss: 0.3602 - accuracy: 0.8391 - val_loss: 0.4181 - val_accuracy: 0.8005\n",
      "Epoch 11/15\n",
      "250/250 [==============================] - 118s 474ms/step - loss: 0.3500 - accuracy: 0.8440 - val_loss: 0.4119 - val_accuracy: 0.8135\n",
      "Epoch 12/15\n",
      "250/250 [==============================] - 117s 467ms/step - loss: 0.3437 - accuracy: 0.8466 - val_loss: 0.4043 - val_accuracy: 0.8125\n",
      "Epoch 13/15\n",
      "250/250 [==============================] - 116s 464ms/step - loss: 0.3319 - accuracy: 0.8535 - val_loss: 0.4056 - val_accuracy: 0.8170\n",
      "Epoch 14/15\n",
      "250/250 [==============================] - 113s 450ms/step - loss: 0.3307 - accuracy: 0.8510 - val_loss: 0.4283 - val_accuracy: 0.8015\n",
      "Epoch 15/15\n",
      "250/250 [==============================] - 113s 450ms/step - loss: 0.3179 - accuracy: 0.8591 - val_loss: 0.4260 - val_accuracy: 0.8170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15b43681610>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################# training and testing  ######################\n",
    "\n",
    "#freeze the convo base\n",
    "conv_base.trainable = False\n",
    "\n",
    "#compile\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#training and testing\n",
    "model.fit_generator(training_set, validation_data=validation_set, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to classify a single image\n",
    "def classify(img) :\n",
    "\tprediction = model.predict(img) #0 or 1\n",
    "\n",
    "\t#training_set.class_indices # {cats : 0 , dogs : 1}\n",
    "\tif prediction[0][0] > 0.5:\n",
    "\t\tprediction = 'dog'\n",
    "\telse:\n",
    "\t\tprediction = 'cat'\n",
    "\treturn prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
